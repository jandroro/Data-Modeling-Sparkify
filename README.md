# Sparkify - Data Modeling and ETL Pipeline

The main objective of the project is to model a database schema in Postgres and create an ETL pipeline using Python for a startup called Sparkify, which wants to analyze and query the data about songs and user activity (logs) from its new music streaming app in an easy way, since nowadays they have their information only in JSON files.

## Analytical objectives

Provide consolidated and structured data to the analytics team so that they can easily query it and understand the behavior of users within the Sparkify music streaming app and finally make decisions. For example: knowing which songs are most listened to by users.

## Project Datasets

### Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![Log example](/assets/log-data.png)

## Database Schema

The project data model has been built under a **star schema**, which contains a fact table and 4 dimension tables. The reason why it was decided to work under this type of scheme was that being a type of scheme that allows us to perform simple queries and quick aggregations, it would greatly facilitate the task of the Sparkify analytics team when consulting the data.

![ER_Diagram](/assets/sparkifydb_erd.png)

### Fact Table
```
songplays: Store the records in log data associated with song plays i.e. records with page "NextSong"
    - songplay_id SERIAL PRIMARY KEY
    - start_time TIMESTAMP
    - user_id INT
    - level VARCHAR
    - song_id VARCHAR
    - artist_id VARCHAR
    - session_id INT
    - location VARCHAR
    - user_agent VARCHAR
```

### Dimension Tables
```
users: Store users in the app
    - user_id INT PRIMARY KEY
    - first_name VARCHAR
    - last_name VARCHAR
    - gender VARCHAR
    - level VARCHAR

songs: Store songs in music database
    - song_id VARCHAR PRIMARY KEY
    - title VARCHAR
    - artist_id VARCHAR
    - year INT
    - duration DECIMAL

artists: Store artists in music database
    - artist_id VARCHAR PRIMARY KEY
    - name VARCHAR
    - location VARCHAR
    - latitude DECIMAL
    - longitude DECIMAL

time: Store timestamps of records in songplays broken down into specific units
    - start_time TIMESTAMP PRIMARY KEY
    - hour INT
    - day INT
    - week INT
    - month INT
    - year INT
    - weekday INT
```

## Project Structure

```
|____data			# Contains Dataset for user activity (logs) and songs
| |____log_data
| | |____...
| |____song_data
| | |____...
|
|____assets			# Images used for README file
| |____...
|
|____etl.ipynb			# ETL builder for development
|____test.ipynb			# Testing ETL and validate results
|____sparkify.ipynb			# Create the data model and run the ETL
|
|____etl.py			# ETL builder
|____sql_queries.py			# Define query structure
|____create_tables.py			# Database/table creation script
|____er_diagram.py			# ER diagram creation script
|
|____README.md			# README file
```

In addition to the data files (data folder), the project workspace includes 8 files:

1. ```test.ipynb```: Display the first few rows of each table to let us check y database.
2. ```create_tables.py```: Drop and create our tables. We run this file to reset our tables before each time we run our ETL scripts.
3. ```etl.ipynb```: Read and process a single file from song_data and log_data folder and loads the data into our tables. This notebook contains detailed instructions on the ETL process for each of the tables.
4. ```etl.py```: Read and process files from song_data and log_data folder and loads them into our tables. We can fill this out based on our work in the ETL notebook.
5. ```sql_queries.py```: Contains all our sql queries, and is imported into the last three files above.
6. ```sparkify.ipynb```: Run the create_tables.py and etl.py files to create our sparkifydb database and tables, and run our ETL process respectively.
7. ```er_diagram.py```: Create a ER diagram based on Postgres tables using sqlalchemy and Python.
8. ```README.md```: Provide discussion on our project.

## ETL Pipeline

The first step in the ETL process is to connect to the ```sparkifydb``` database and create a cursor that allows us to execute our queries throughout the entire process. Next we read and process all the song files contained in the folder ```data/song_data```, writing the processed information into the dimension tables ```songs``` and ```artists```. After that we read and process all the log files contained in the folder ```data/log_data``` to finally save this information inside the dimension tables ```time``` and ```users```, as well as in the fact table ```songplays```

## Program execution

To run the project in a local environment, you must first meet the following requirements:

1. Have Postgres installed.
2. Have a Postgres user with administrator access.
3. Have Python 3.x installed
4. Make sure we have the Python psycopg2 and pandas packages installed.
5. In case you want to run the project from Jupyter Notebook, make sure you have installed a tool that allows you to run notebooks (Anaconda for example)

Once the aforementioned requirements have been validated, we follow the following steps to execute the project:

1. Modify the connection string to Postgres within the files ```etl.ipynb```, ```etl.py```, ```create_tables.py``` and ```test.ipynb```, placing the accesses that you have previously created to connect to Postgres.
2. Open the terminal, go to the path where the project is located and execute the following:
```
python3 create_tables.py
python3 etl.py
```
3. In case we want to run our ETL pipeline through Jupyter Notebook, we run the ```etl.ipynb``` file instead of step 2.
4. Finally, we run the ```test.ipynb``` notebook to validate that our data has been inserted correctly, or we can also directly query the tables of our sparkifydb database in Postgres.

## Generate ER diagram

Additionally, you can create an ER diagram on the tables created in Postgres by running the er_diagram.py file. To do this, you must follow the following steps:

1. Install the following packages:
```
pip install SQLAlchemy
pip install sqlalchemy_schemadisplay
```
2. Open the terminal, go to the path where the project is located and execute the following:
```
python3 er_diagram.py
```